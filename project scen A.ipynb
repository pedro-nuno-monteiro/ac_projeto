{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario A - Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Import de todas as bibliotecas necessárias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Leitura do local dataframe + informações básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('RTA Dataset.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# vários df's para as diferentes experiências\n",
    "df_1 = df.copy()\n",
    "df_2 = df.copy()\n",
    "df_3 = df.copy()\n",
    "df_4 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descrever categorical columns\n",
    "df.describe(include = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    value_counts = df[col].value_counts(normalize=True)\n",
    "    unique_values[col] = [f\"{value}: {count*100:.2f}%\" for value, count in zip(value_counts.index, value_counts.values)]\n",
    "\n",
    "print(tabulate(unique_values, headers = 'keys', tablefmt = 'orgtbl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()   #verificar quantas colunas têm valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - pre processing + divisão por experiências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de repartir em experiências, vamos analisar o que significam algumas colunas que aparentam não ser tão importantes para a nossa análise, eliminando-as de qualquer experiência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_vehicle = df['Type_of_vehicle'].value_counts()    #ELIMINAR COM CERTEZA -> irrelevante\n",
    "print(type_of_vehicle, \"\\n\\n\")\n",
    "\n",
    "owner_of_vehicle = df['Owner_of_vehicle'].value_counts()  #ELIMINAR COM CERTEZA -> irrelevante\n",
    "print(owner_of_vehicle, \"\\n\\n\")\n",
    "\n",
    "service_year_of_vehicle = df['Service_year_of_vehicle'].value_counts()    #ELIMINAR COM CERTEZA -> muitos nans\n",
    "print(service_year_of_vehicle, \"\\n\\n\")\n",
    "\n",
    "defect_of_vehicle = df['Defect_of_vehicle'].value_counts()  #tem 7777 \"no defect\"\n",
    "print(defect_of_vehicle, \"\\n\\n\")    #ELIMINAR -> DADOS CONFUSOS\n",
    "\n",
    "area_accident_occured = df['Area_accident_occured'].value_counts() #ELIMINAR -> IRRELEVANTE\n",
    "print(area_accident_occured, \"\\n\\n\")\n",
    "\n",
    "road_allignment = df['Road_allignment'].value_counts()  #ELIMINAR -> muitos dados iguais -> +80%  \n",
    "print(road_allignment, \"\\n\\n\")\n",
    "\n",
    "pedestrian_movement = df['Pedestrian_movement'].value_counts()  #ELIMINAR -> dados sem sentido\n",
    "print(pedestrian_movement, \"\\n\\n\")\n",
    "\n",
    "fitness_of_casuality = df['Fitness_of_casuality'].value_counts()  #ELIMINAR -> faltam dados e muitos iguais\n",
    "print(fitness_of_casuality, \"\\n\\n\")\n",
    "\n",
    "work_of_causalty = df['Work_of_casuality'].value_counts()\n",
    "print(work_of_causalty, \"\\n\\n\")\n",
    "\n",
    "casualty_class = df['Casualty_class'].value_counts()  #ELIMINAR -> ver matriz de correlação\n",
    "print(casualty_class, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a análise, eliminamos as colunas que afetam pouco a nossa saída, que, como sabemos, é a última coluna: \"Gravidade do Acidente\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_a_eliminar = ['Type_of_vehicle', 'Owner_of_vehicle', 'Defect_of_vehicle', 'Area_accident_occured',\n",
    "                        'Road_allignment', 'Pedestrian_movement', 'Fitness_of_casuality', 'Work_of_casuality']\n",
    "\n",
    "df.drop(colunas_a_eliminar, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, para uma melhor compreensão da tabela, vamos renomear as colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time, day_of_week, age_band_of_driver, sex_of_driver, educational_level, vehicle_driver_relation, driving_experience\n",
    "#type_of_vehicle, owner_of_vehicle, service_year_of_vehicle, defect_of_vehicle, area_accident_occured\n",
    "#lanes_or_medians, road_allignment, types_of_junction, road_surface_type, road_surface_conditions, ligh_conditions\n",
    "#weather_conditions, type_of_collision, number_of_vehicles_involved, number_of_casualties, vehicle_movement\n",
    "#casualty_class, sex_of_casualty, age_band_of_casualty, casualty_severity, work_of_casualty, fitness_of_casualty\n",
    "#pedestrian_movement, cause_of_accident, accident_severity\n",
    "\n",
    "colunas_renomeadas = ['Período', 'Dia de Semana', 'Faixa Etária', 'Género', 'Nível de Educação', 'Relação com o Veículo', \n",
    "                    'Experiência de Condução', 'Idade do Veículo', 'Situação de Faixa', 'Tipo de Cruzamento',\n",
    "                    'Tipo de Estrada', 'Condições do Piso', 'Condições de Visibilidade',\n",
    "                    'Condições Meteorológicas', 'Tipo de Colisão', 'N.º Veículos Envolvidos',\n",
    "                    'Número de Vítimas', 'Movimento do Veículo', 'Tipo de Vítima','Género da Vítima',\n",
    "                    'Faixa Etária da Vítima', 'Gravidade da Vítima',\n",
    "                    'Causa do Acidente', 'Gravidade do Acidente']\n",
    "\n",
    "df.columns = colunas_renomeadas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora perceber quantos valores nulos existem e ainda realizar a substituição desses mesmos pela moda da coluna.\n",
    "\n",
    "Neste passo, fomos apenar substituir os valores nulos cujas colunas tinham menos de 1000 valores \"NaN\", uma vez que afeta pouco a proporção dos valores, servindo de base a todas as experiências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('na', pd.NA, inplace=True) # substituir \"na\" por um valor realmente nulo\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Nível de Educação'] = df['Nível de Educação'].fillna(df['Nível de Educação'].mode()[0])\n",
    "df['Relação com o Veículo'] = df['Relação com o Veículo'].fillna(df['Relação com o Veículo'].mode()[0])\n",
    "df['Experiência de Condução'] = df['Experiência de Condução'].fillna(df['Experiência de Condução'].mode()[0])\n",
    "df['Situação de Faixa'] = df['Situação de Faixa'].fillna(df['Situação de Faixa'].mode()[0])\n",
    "df['Tipo de Cruzamento'] = df['Tipo de Cruzamento'].fillna(df['Tipo de Cruzamento'].mode()[0])\n",
    "df['Tipo de Estrada'] = df['Tipo de Estrada'].fillna(df['Tipo de Estrada'].mode()[0])\n",
    "df['Tipo de Colisão'] = df['Tipo de Colisão'].fillna(df['Tipo de Colisão'].mode()[0])\n",
    "df['Movimento do Veículo'] = df['Movimento do Veículo'].fillna(df['Movimento do Veículo'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    if col != 'Período':\n",
    "        value_counts = df[col].value_counts(normalize=True)\n",
    "        unique_values[col] = [f\"{value}: {count*100:.2f}%\" for value, count in zip(value_counts.index, value_counts.values)]\n",
    "\n",
    "print(tabulate(unique_values, headers = 'keys', tablefmt = 'orgtbl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary target + mapping ordenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizar_periodo(time):\n",
    "    hour = time.hour\n",
    "    if hour >= 0 and hour < 7:\n",
    "        return \"Dawn\"\n",
    "    elif hour >= 7 and hour < 12:\n",
    "        return \"Morning\"\n",
    "    elif hour >= 12 and hour < 17:\n",
    "        return \"Afternoon\"\n",
    "    elif hour >= 17 and hour < 20:\n",
    "        return \"Dusk\"\n",
    "    elif hour >= 20 and hour <= 24:\n",
    "        return \"Night\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faixa_etaria_mapping = {'18-30': 2, '31-50': 3, 'Over 51': 4, 'Unknown': 0, 'Under 18': 1}\n",
    "genero_mapping = {'Male': 1, 'Female': 2, 'Unknown': 0}\n",
    "nivel_educacao_mapping = {'Junior high school': 5, 'Elementary school': 4, 'High school': 5, 'Above high school': 6,\n",
    "                            'Writing & reading': 3, 'Unknown': 0, 'Illiterate': 1}\n",
    "relacao_com_veiculo_mapping = {'Employee': 3, 'Owner': 2, 'Other': 1, 'Unknown': 0}\n",
    "experiencia_conducao_mapping = {'5-10yr': 5, '2-5yr': 4, 'Above 10yr': 6, '1-2yr': 3, 'Below 1yr': 2, 'No Licence': 1, 'unknown': 0}\n",
    "gravidade_acidente_mapping = {'Slight Injury': 0, 'Serious Injury': 1, 'Fatal injury': 1}\n",
    "\n",
    "df['Período'] = pd.to_datetime(df['Período'], format = '%H:%M:%S')\n",
    "df[\"Período\"] = df[\"Período\"].map(categorizar_periodo)\n",
    "\n",
    "periodo_mapping = {'Dawn': 0, 'Morning': 1, 'Afternoon': 2, 'Dusk': 3, 'Night': 4}\n",
    "\n",
    "df['Faixa Etária'] = df['Faixa Etária'].map(faixa_etaria_mapping)\n",
    "df['Género'] = df['Género'].map(genero_mapping)\n",
    "df['Nível de Educação'] = df['Nível de Educação'].map(nivel_educacao_mapping)\n",
    "df['Relação com o Veículo'] = df['Relação com o Veículo'].map(relacao_com_veiculo_mapping)\n",
    "df['Experiência de Condução'] = df['Experiência de Condução'].map(experiencia_conducao_mapping)\n",
    "df['Gravidade do Acidente'] = df['Gravidade do Acidente'].map(gravidade_acidente_mapping)\n",
    "df['Período'] = df['Período'].map(periodo_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = ['Situação de Faixa', 'Tipo de Cruzamento', 'Tipo de Estrada', \n",
    "                    'Condições do Piso', 'Condições de Visibilidade', \n",
    "                    'Condições Meteorológicas', 'Tipo de Colisão', \n",
    "                    'Movimento do Veículo', 'Causa do Acidente']\n",
    "\n",
    "# fazer enconder das que não se podem ordenar\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in columns_to_encode:\n",
    "    df[column] = label_encoder.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    value_counts = df[col].value_counts(normalize=True)\n",
    "    unique_values[col] = [f\"{value}: {count*100:.2f}%\" for value, count in zip(value_counts.index, value_counts.values)]\n",
    "\n",
    "print(tabulate(unique_values, headers = 'keys', tablefmt = 'orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.copy()\n",
    "df_2 = df.copy()\n",
    "df_3 = df.copy()\n",
    "df_4 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divisão por experiências:\n",
    "\n",
    " *  exp 1 - distribuir a proporção dos valores nulos (>4000) pelos restantes valores, mantendo a proporção\n",
    " *  exp 2 - eliminar colunas com valores nulos > 3500\n",
    " *  exp 3 - substituir valores nulos por valores em que a coluna da gravidade é a mesma (tendo em conta a sua moda)\n",
    " *  exp 4 - aplicar método do KNN às colunas com valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.1 - distribuir a proporção dos valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos as colunas que queremos alterar\n",
    "colunas_a_substituir = ['Idade do Veículo', 'Tipo de Vítima', 'Género da Vítima', 'Faixa Etária da Vítima', 'Gravidade da Vítima']\n",
    "\n",
    "for coluna in colunas_a_substituir:\n",
    "\n",
    "    no_valores_nulos = df_1[coluna].isnull().sum()\n",
    "\n",
    "    # proporção de valores não nulos para cada categoria\n",
    "    proporcoes_categoria = df_1[coluna].value_counts(normalize = True)\n",
    "\n",
    "    # calcular o número de valores nulos a serem distribuídos para cada categoria\n",
    "    distribuicao_nulos = (proporcoes_categoria * no_valores_nulos).round().astype(int)\n",
    "\n",
    "    # neste caso, damos assign a cada valor nulo de forma RANDOM\n",
    "    indices_nulo = df_1[df_1[coluna].isnull()].index\n",
    "    for category, count in distribuicao_nulos.items():\n",
    "        sample_indices = np.random.choice(indices_nulo, size = count, replace = False)\n",
    "        df_1.loc[sample_indices, coluna] = category\n",
    "\n",
    "    # preencher algum valor nulo que falte, RANDOM\n",
    "    remaining_null_indices = df_1[df_1[coluna].isnull()].index\n",
    "    remaining_categories = list(proporcoes_categoria.index)\n",
    "    for index in remaining_null_indices:\n",
    "        df_1.at[index, coluna] = np.random.choice(remaining_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_values = {}\n",
    "for column in df_1.columns:\n",
    "    counts = df_1[column].value_counts(normalize = True, dropna = False) * 100\n",
    "    percentage_values[column] = counts\n",
    "\n",
    "for column, percentages in percentage_values.items():\n",
    "    print(percentages)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.2 - eliminar colunas com valores nulos > 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_valores_nulos = ['Idade do Veículo', 'Tipo de Vítima', 'Género da Vítima', 'Faixa Etária da Vítima', 'Gravidade da Vítima']\n",
    "\n",
    "df_2.drop(colunas_valores_nulos, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.3 - substituir valores nulos por valores em que a coluna da gravidade é a mesma (tendo em conta a sua moda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos as colunas que queremos alterar\n",
    "colunas_a_substituir = ['Idade do Veículo', 'Tipo de Vítima', 'Género da Vítima', 'Faixa Etária da Vítima', 'Gravidade da Vítima']\n",
    "\n",
    "percentage_values = {}\n",
    "for column in colunas_a_substituir:\n",
    "    counts = df_3[column].value_counts(normalize = True, dropna = False) * 100\n",
    "    percentage_values[column] = counts\n",
    "\n",
    "for column, percentages in percentage_values.items():\n",
    "    print(percentages)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target column\n",
    "target_column = 'Gravidade do Acidente'\n",
    "\n",
    "# Iterate over each column with null values\n",
    "for column in colunas_a_substituir:\n",
    "    \n",
    "    # Calculate the proportions of each value in the column for each category in the target column\n",
    "    proportions = df_3.groupby(target_column)[column].value_counts(normalize=True)\n",
    "    \n",
    "    # For each null value in the column, replace it based on the proportions\n",
    "    for index, row in df_3[df_3[column].isnull()].iterrows():\n",
    "        \n",
    "        # Get the proportions for the target value of this row\n",
    "        target_value = row[target_column]\n",
    "        target_proportions = proportions[target_value]\n",
    "        \n",
    "        # Sample from the proportions to replace the null value\n",
    "        new_value = target_proportions.sample(weights=target_proportions).index[0]\n",
    "        \n",
    "        # Replace the null value\n",
    "        df_3.at[index, column] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_values = {}\n",
    "for column in colunas_a_substituir:\n",
    "    counts = df_3[column].value_counts(normalize = True, dropna = False) * 100\n",
    "    percentage_values[column] = counts\n",
    "\n",
    "for column, percentages in percentage_values.items():\n",
    "    print(percentages)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.4 - aplicar método do KNN às colunas com valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer()\n",
    "\n",
    "# Get the columns with null values\n",
    "colunas_a_substituir = df_4.columns[df_4.isnull().any()]\n",
    "\n",
    "# Iterate over each column with null values\n",
    "for column in colunas_a_substituir:\n",
    "    # If the column is categorical, encode it using one-hot encoding or ordinal encoding\n",
    "    if df_4[column].dtype == 'object':\n",
    "        # Convert 'NAType' to NaN\n",
    "        df_4[column] = df_4[column].replace('NAType', np.nan)\n",
    "        # Handle missing values with a placeholder\n",
    "        df_4[column] = df_4[column].fillna('missing')\n",
    "        \n",
    "        # Apply ordinal encoding\n",
    "        encoder = OrdinalEncoder()\n",
    "        encoded_values = encoder.fit_transform(df_4[[column]])\n",
    "        encoded_df = pd.DataFrame(encoded_values, columns=[column], index=df_4.index)\n",
    "        # Drop the original categorical column and concatenate the encoded columns\n",
    "        df_4 = pd.concat([df_4.drop(column, axis=1), encoded_df], axis=1)\n",
    "    else:\n",
    "        # Extract the column data\n",
    "        X = df_4.dropna(subset=[column], axis=0).drop(colunas_a_substituir, axis=1)\n",
    "        y = df_4.dropna(subset=[column], axis=0)[column]\n",
    "        X_with_null = df_4[df_4[column].isnull()].drop(colunas_a_substituir, axis=1)\n",
    "        \n",
    "        # Apply KNN imputation\n",
    "        imputer.fit(X, y)\n",
    "        imputed_values = imputer.transform(X_with_null)\n",
    "        \n",
    "        # Update the dataframe with imputed values\n",
    "        df_4.loc[X_with_null.index, column] = imputed_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_values = {}\n",
    "for column in df_4.columns:\n",
    "    counts = df_4[column].value_counts(normalize = True, dropna = False) * 100\n",
    "    percentage_values[column] = counts\n",
    "\n",
    "for column, percentages in percentage_values.items():\n",
    "    print(percentages)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Fazer matriz de correlação e importância de variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o pré-processamento, decidimos ficar com a terceira experiência, pois é a que apresenta melhores resultados, como podemos observar através da matriz de correlação entre as variáveis.\n",
    "\n",
    "Aplicamos ainda o mapping às colunas anteriormente nulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_3.copy()\n",
    "\n",
    "idade_do_veiculo_mapping = {'Unknown': 0, '2-5yrs': 3, 'Above 10yr': 5, '5-10yrs': 4 , '1-2yr': 2, 'Below 1yr': 1}\n",
    "tipo_vitima_mapping = {'Driver or rider': 1, 'Pedestrian': 2, 'Passenger': 3}\n",
    "genero_vitima_mapping = {'Male': 1, 'Female': 0}\n",
    "faixa_etaria_vitima_mapping = {'18-30': 3, '31-50': 4, 'Under 18': 2, 'Over 51': 5, '5': 1}\n",
    "\n",
    "df['Idade do Veículo'] = df['Idade do Veículo'].map(idade_do_veiculo_mapping)\n",
    "df['Tipo de Vítima'] = df['Tipo de Vítima'].map(tipo_vitima_mapping)\n",
    "df['Género da Vítima'] = df['Género da Vítima'].map(genero_vitima_mapping)\n",
    "df['Faixa Etária da Vítima'] = df['Faixa Etária da Vítima'].map(faixa_etaria_vitima_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize = (20, 15))\n",
    "sns.heatmap(correlation_matrix, annot = True, cmap = 'vlag', fmt = \".3f\", linewidths = 1, square=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida, analisamos a importância de cada feature para o modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values # todas as features\n",
    "Y =  df.iloc[:, -1].values # feature target\n",
    "\n",
    "modelo = ExtraTreesClassifier()\n",
    "modelo.fit(X, Y)\n",
    "\n",
    "feature_importances = pd.Series(modelo.feature_importances_, index = df.columns[:-1])\n",
    "feature_importances.nlargest(10).plot(kind='barh')  # mostrar as 10 features mais importantes\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste ponto, dividimos os dados em sets de treino e de teste, utilizando 30% dos dados para teste e os restantes para teino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.30, random_state = 30)\n",
    "\n",
    "print(\"Número de exemplos nos dados de treino: \", X_train.shape[0])\n",
    "print(\"Número de exemplos nos dados de teste: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 - Árvores de Decisão"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro modelo de machine learning utilizado foi o modelo de árvores de decisão.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro vamos estanderizar as nossas features. Isso é feito para garantir que todas as características tenham a mesma escala, o que pode melhorar o desempenho do algoritmo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divisão por experiências:\n",
    " *  exp 0 - normal\n",
    " *  exp 1 - ajustando o peso de cada classe\n",
    " *  exp 2 - oversampling\n",
    " *  exp 3 - undersampling\n",
    " *  exp 4 - oversampling + undersampling\n",
    " *  exp 5 - SMOTE\n",
    " *  exp 6 - ADASYN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5.0 - aplicar o método de forma normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O parâmetro max_depth = 4 define a profundidade máxima da árvore de decisão, \n",
    "# visto que limitar a profundidade pode ajudar a evitar overfitting, tornando a árvore mais simples.\n",
    "# critério: usamos a entropia porque é a que maximiza a informação ganha\n",
    "\n",
    "clf_0 = DecisionTreeClassifier(random_state = 42, criterion = \"entropy\", max_depth = 4)\n",
    "clf_0 = clf_0.fit(X_train, y_train)\n",
    "\n",
    "y_pred_0 = clf_0.predict(X_test)\n",
    "y_train_pred_0 = clf_0.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar no classification report, o recall e o f1-score são nulos o que mostra que o modelo não está a ser utilizado de uma forma correta. \n",
    "\n",
    "Como podemos observar, existe desequilíbrio de classe no conjunto de dados, pois a classe '1' é significativamente menor do que as outras classes. Para tentar corrigir este erros, iremos usar técnicas de balanceamento de dados:\n",
    "\n",
    " * *oversampling* - aumentando o número de amostras da classe minoritária;\n",
    " * *undersampling* - reduzindo o número de amostras da classe majoritária, para lidar com o desequilíbrio de classe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quando o train data accuracy é muito maior que o test data accuracy, significa que o modelo está a overfitting, ou seja, está a aprender demasiado bem os dados de treino e não está a generalizar bem para os dados de teste.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5.1 - aplicar o método ajustando os pesos de cada classe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = {0: 1, 1: 3}\n",
    "\n",
    "clf_1 = DecisionTreeClassifier(random_state = 42, criterion = \"entropy\", max_depth = 4, class_weight=class_weights)\n",
    "clf_1 = clf_1.fit(X_train, y_train)\n",
    "\n",
    "# Treino: Arvore de Decisão\n",
    "y_pred_1 = clf_1.predict(X_test)\n",
    "y_train_pred_1 = clf_1.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5.2 - aplicar o método de oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversample = RandomOverSampler(sampling_strategy = 'minority')\n",
    "X_train_resampled, y_train_resampled = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "clf_2 = DecisionTreeClassifier(random_state = 42, criterion = \"entropy\", max_depth = 4)\n",
    "clf_2 = clf_2.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred_2= clf_2.predict(X_test)\n",
    "y_train_pred_2 = clf_2.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5.3 - aplicar o método de undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersample = RandomUnderSampler(sampling_strategy = 'majority')\n",
    "X_train_resampled, y_train_resampled = undersample.fit_resample(X_train, y_train)\n",
    "\n",
    "clf_3 = DecisionTreeClassifier(random_state = 42, criterion = \"entropy\", max_depth = 4)\n",
    "clf_3 = clf_3.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred_3= clf_3.predict(X_test)\n",
    "y_train_pred_3= clf_3.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5.4 - aplicar o método de undersampling e oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um pipeline com oversampling e undersampling\n",
    "pipeline = Pipeline([\n",
    "            ('over', RandomOverSampler(sampling_strategy = 'minority')),\n",
    "            ('under', RandomUnderSampler(sampling_strategy = 'majority')),\n",
    "        ])\n",
    "\n",
    "# Aplicando o pipeline no conjunto de treino\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "clf_4 = DecisionTreeClassifier(random_state = 42, criterion = \"entropy\", max_depth = 4)\n",
    "\n",
    "clf_4.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred_4 = clf_4.predict(X_test)\n",
    "y_train_pred_4 = clf_4.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5.5 - aplicar o método de SMOTE (Synthetic Minority Over-sampling Technique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy = 'auto', random_state = 42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "clf_5 = DecisionTreeClassifier(random_state = 42, criterion = \"entropy\", max_depth = 4)\n",
    "\n",
    "# Treinando o classificador\n",
    "clf_5.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred_5 = clf_5.predict(X_test)\n",
    "y_train_pred_5= clf_5.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 5.6 - aplicar método de ADASYN (Adaptive Synthetic Sampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adasyn = ADASYN(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
    "\n",
    "clf_6 = DecisionTreeClassifier(random_state = 42, criterion = \"entropy\", max_depth = 4)\n",
    "\n",
    "# Treinando o classificador\n",
    "clf_6.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Fazendo previsões\n",
    "y_pred_6 = clf_6.predict(X_test)\n",
    "y_train_pred_6 = clf_6.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora vamos observar os diferentes classification reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print dos valores accuracy\n",
    "y_pred_todos = []\n",
    "y_pred_todos.extend([y_pred_0, y_pred_1, y_pred_2, y_pred_3, y_pred_4, y_pred_5, y_pred_6])\n",
    "\n",
    "y_train_pred_todos=[]\n",
    "y_train_pred_todos.extend([y_train_pred_0, y_train_pred_1, y_train_pred_2, y_train_pred_3, y_train_pred_4, y_train_pred_5, y_train_pred_6])\n",
    "\n",
    "for indice, (y_pred, y_train_pred) in enumerate(zip(y_pred_todos, y_train_pred_todos)):\n",
    "    print('\\n\\tClassification_report -', indice, 'Experiência\\n')\n",
    "    print('Train data accuracy: ', accuracy_score(y_true = y_train, y_pred = y_train_pred))\n",
    "    print('Test data accuracy: ', accuracy_score(y_true = y_test, y_pred = y_pred))\n",
    "    print('Decision tree accuracy: ', accuracy_score(y_pred, y_test))\n",
    "    print('\\n')\n",
    "    print(classification_report(y_test, y_pred, zero_division = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JÁ NÃO É > CORRIGIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após estas várias experiências, decidimos que o método que apresenta melhores resultados é a Experiência I, por isso agora vamos plotar a àrvore de decisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "tree.plot_tree(clf_1, fontsize = 8, feature_names = df.columns[:-1], filled = True, rounded = True, proportion = True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após realizar um classification report, vamos agora mostrar a matriz de confusão. Assim podemos ver a quantidade de falsos positivos e falsos negativos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, y_pred_1)\n",
    "\n",
    "ax = sns.heatmap(cf_matrix, annot = True, cmap = 'seismic', fmt = \"4.0f\")\n",
    "ax.set_title('Seaborn Confusion Matrix\\n')\n",
    "ax.set_xlabel('\\nPredicted decision case disposition')\n",
    "ax.set_ylabel('Actual decision case disposition\\n')\n",
    "\n",
    "ax.xaxis.set_ticklabels(['Slight Injury', 'Not Slight Injury'])\n",
    "ax.yaxis.set_ticklabels(['Slight Injury', 'Not Slight Injury'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6 - KNN Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iremos, por fim, aplicar o método KNN Classifier ao nosso dataframe, através de 2 experiências\n",
    "\n",
    " * exp 1 - método normal\n",
    " * exp 2 - reaplicar o método 20 vezes\n",
    " * exp 3 - aplicar o método de *oversampling* e *undersampling* junto do KNN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 6.1 - normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_1 = KNeighborsClassifier()\n",
    " \n",
    "knn_1.fit(X_train, y_train)\n",
    " \n",
    "y_pred_knn_1 = knn_1.predict(X_test)\n",
    "y_train_pred_knn_1 = knn_1.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 6.2 - método 20 vezes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos calcular o erro médio para os valores de k entre 1 e 20\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "erro = []\n",
    "\n",
    "for i in range(1, 21):\n",
    "    knnImprove = KNeighborsClassifier(n_neighbors = i)\n",
    "    knnImprove.fit(X_train, y_train)\n",
    "    pred_i = knnImprove.predict(X_test)\n",
    "    erro.append(np.mean(pred_i != y_test))\n",
    "    if i == 20:\n",
    "        print(f\"Erro médio para k = {i}: {erro[-1]}\")\n",
    "\n",
    "plt.figure(figsize = (20, 6))\n",
    "plt.plot(range(1, 21), erro, color = 'blue', linestyle = '-', marker = 'o', markerfacecolor = 'cyan', markersize = 12, markeredgewidth = 2, linewidth = 2)\n",
    "plt.title('% de erro em relação ao valor de K')\n",
    "plt.xlabel('Valor de K (n_neighbors)')\n",
    "plt.ylabel('% de erro médio')\n",
    "plt.show()\n",
    "\n",
    "scores = []\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "for i in range(1, 21):\n",
    "    knnAccuracy = KNeighborsClassifier(n_neighbors = i)\n",
    "    score = cross_val_score(knnAccuracy, X, Y, cv = 5)\n",
    "    scores.append(np.mean(score))\n",
    "\n",
    "plt.figure(figsize = (20, 6))\n",
    "sns.lineplot(x = range(1, 21), y = scores, color = 'red', linestyle = '-', marker = 'o', markerfacecolor = 'magenta', markersize = 12, markeredgewidth = 2, linewidth = 2)\n",
    "plt.xlabel(\"K Values\")\n",
    "plt.ylabel(\"Accuracy Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Novo modelo do KNN com o valor de K = 20\n",
    "knn_2 = KNeighborsClassifier(n_neighbors = 20)\n",
    "knn_2.fit(X_train, y_train)\n",
    "\n",
    "y_pred_knn_2 = knn_2.predict(X_test)\n",
    "y_train_pred_knn_2 = knn_2.predict(X_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 6.3 - *oversampling* + *undersampling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "            ('over', RandomOverSampler(sampling_strategy = 'minority')),\n",
    "            ('under', RandomUnderSampler(sampling_strategy = 'majority')),\n",
    "        ])\n",
    "\n",
    "X_train_resampled, y_train_resampled = pipeline.fit_resample(X_train, y_train)\n",
    "\n",
    "knn_3 = KNeighborsClassifier()\n",
    "knn_3 = knn_3.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "y_pred_knn_3 = knn_3.predict(X_test)\n",
    "y_train_pred_knn_3 = knn_3.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observamos agora os 3 classification reports das 3 experiêncas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_todos = []\n",
    "y_pred_todos.extend([y_pred_knn_1, y_pred_knn_2, y_pred_knn_3])\n",
    "\n",
    "y_train_pred_todos=[]\n",
    "y_train_pred_todos.extend([y_train_pred_knn_1, y_train_pred_knn_2, y_train_pred_knn_3])\n",
    "\n",
    "for indice, (y_pred, y_train_pred) in enumerate(zip(y_pred_todos, y_train_pred_todos)):\n",
    "    print('\\n\\tClassification_report -', indice + 1, 'Experiência\\n')\n",
    "    print('Train data accuracy: ', accuracy_score(y_true = y_train, y_pred = y_train_pred))\n",
    "    print('Test data accuracy: ', accuracy_score(y_true = y_test, y_pred = y_pred))\n",
    "    print('KNN Classifier accuracy: ', accuracy_score(y_pred, y_test))\n",
    "    print()\n",
    "    print(classification_report(y_test, y_pred, zero_division = 1))\n",
    "\n",
    "    cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax_1 = sns.heatmap(cf_matrix, annot=True, fmt=\"d\", cmap='Blues', cbar=False)\n",
    "    plt.title(f'Confusion Matrix - Experience {indice + 1}')\n",
    "    ax_1.set_xlabel('\\nPredicted decision case disposition')\n",
    "    ax_1.set_ylabel('Actual decision case disposition\\n')\n",
    "\n",
    "    ax_1.xaxis.set_ticklabels(['Slight Injury', 'Not Slight Injury'])\n",
    "    ax_1.yaxis.set_ticklabels(['Slight Injury', 'Not Slight Injury'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
