{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario A - Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 - Import de todas as bibliotecas necessárias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from tabulate import tabulate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Leitura do local dataframe + informações básicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('RTA Dataset.csv')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# vários df's para as diferentes experiências\n",
    "df_1 = df.copy()\n",
    "df_2 = df.copy()\n",
    "df_3 = df.copy()\n",
    "df_4 = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# descrever categorical columns\n",
    "df.describe(include = 'object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    value_counts = df[col].value_counts(normalize=True)\n",
    "    unique_values[col] = [f\"{value}: {count*100:.2f}%\" for value, count in zip(value_counts.index, value_counts.values)]\n",
    "\n",
    "print(tabulate(unique_values, headers = 'keys', tablefmt = 'orgtbl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()   #verificar quantas colunas têm valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - pre processing + divisão por experiências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de repartir em experiências, vamos analisar o que significam algumas colunas que aparentam não ser tão importantes para a nossa análise, eliminando-as de qualquer experiência"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_of_vehicle = df['Type_of_vehicle'].value_counts()    #ELIMINAR COM CERTEZA -> irrelevante\n",
    "print(type_of_vehicle, \"\\n\\n\")\n",
    "\n",
    "owner_of_vehicle = df['Owner_of_vehicle'].value_counts()  #ELIMINAR COM CERTEZA -> irrelevante\n",
    "print(owner_of_vehicle, \"\\n\\n\")\n",
    "\n",
    "service_year_of_vehicle = df['Service_year_of_vehicle'].value_counts()    #ELIMINAR COM CERTEZA -> muitos nans\n",
    "print(service_year_of_vehicle, \"\\n\\n\")\n",
    "\n",
    "defect_of_vehicle = df['Defect_of_vehicle'].value_counts()  #tem 7777 \"no defect\"\n",
    "print(defect_of_vehicle, \"\\n\\n\")    #ELIMINAR -> DADOS CONFUSOS\n",
    "\n",
    "area_accident_occured = df['Area_accident_occured'].value_counts() #ELIMINAR -> IRRELEVANTE\n",
    "print(area_accident_occured, \"\\n\\n\")\n",
    "\n",
    "road_allignment = df['Road_allignment'].value_counts()  #ELIMINAR -> muitos dados iguais -> +80%  \n",
    "print(road_allignment, \"\\n\\n\")\n",
    "\n",
    "pedestrian_movement = df['Pedestrian_movement'].value_counts()  #ELIMINAR -> dados sem sentido\n",
    "print(pedestrian_movement, \"\\n\\n\")\n",
    "\n",
    "fitness_of_casuality = df['Fitness_of_casuality'].value_counts()  #ELIMINAR -> faltam dados e muitos iguais\n",
    "print(fitness_of_casuality, \"\\n\\n\")\n",
    "\n",
    "work_of_causalty = df['Work_of_casuality'].value_counts()\n",
    "print(work_of_causalty, \"\\n\\n\")\n",
    "\n",
    "casualty_class = df['Casualty_class'].value_counts()  #ELIMINAR -> ver matriz de correlação\n",
    "print(casualty_class, \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a análise, eliminamos as colunas que afetam pouco a nossa saída, que, como sabemos, é a última coluna: \"Gravidade do Acidente\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_a_eliminar = ['Type_of_vehicle', 'Owner_of_vehicle', 'Defect_of_vehicle', 'Area_accident_occured',\n",
    "                        'Road_allignment', 'Pedestrian_movement', 'Fitness_of_casuality', 'Work_of_casuality']\n",
    "\n",
    "df.drop(colunas_a_eliminar, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, para uma melhor compreensão da tabela, vamos renomear as colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#time, day_of_week, age_band_of_driver, sex_of_driver, educational_level, vehicle_driver_relation, driving_experience\n",
    "#type_of_vehicle, owner_of_vehicle, service_year_of_vehicle, defect_of_vehicle, area_accident_occured\n",
    "#lanes_or_medians, road_allignment, types_of_junction, road_surface_type, road_surface_conditions, ligh_conditions\n",
    "#weather_conditions, type_of_collision, number_of_vehicles_involved, number_of_casualties, vehicle_movement\n",
    "#casualty_class, sex_of_casualty, age_band_of_casualty, casualty_severity, work_of_casualty, fitness_of_casualty\n",
    "#pedestrian_movement, cause_of_accident, accident_severity\n",
    "\n",
    "colunas_renomeadas = ['Período', 'Dia de Semana', 'Faixa Etária', 'Género', 'Nível de Educação', 'Relação com o Veículo', \n",
    "                    'Experiência de Condução', 'Idade do Veículo', 'Situação de Faixa', 'Tipo de Cruzamento',\n",
    "                    'Tipo de Estrada', 'Condições do Piso', 'Condições de Visibilidade',\n",
    "                    'Condições Meteorológicas', 'Tipo de Colisão', 'N.º Veículos Envolvidos',\n",
    "                    'Número de Vítimas', 'Movimento do Veículo', 'Tipo de Vítima','Género da Vítima',\n",
    "                    'Faixa Etária da Vítima', 'Gravidade da Vítima',\n",
    "                    'Causa do Acidente', 'Gravidade do Acidente']\n",
    "\n",
    "df.columns = colunas_renomeadas\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora perceber quantos valores nulos existem e ainda realizar a substituição desses mesmos pela moda da coluna.\n",
    "\n",
    "Neste passo, fomos apenar substituir os valores nulos cujas colunas tinham menos de 1000 valores \"NaN\", uma vez que afeta pouco a proporção dos valores, servindo de base a todas as experiências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.replace('na', pd.NA, inplace=True) # substituir \"na\" por um valor realmente nulo\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Nível de Educação'] = df['Nível de Educação'].fillna(df['Nível de Educação'].mode()[0])\n",
    "df['Relação com o Veículo'] = df['Relação com o Veículo'].fillna(df['Relação com o Veículo'].mode()[0])\n",
    "df['Experiência de Condução'] = df['Experiência de Condução'].fillna(df['Experiência de Condução'].mode()[0])\n",
    "df['Situação de Faixa'] = df['Situação de Faixa'].fillna(df['Situação de Faixa'].mode()[0])\n",
    "df['Tipo de Cruzamento'] = df['Tipo de Cruzamento'].fillna(df['Tipo de Cruzamento'].mode()[0])\n",
    "df['Tipo de Estrada'] = df['Tipo de Estrada'].fillna(df['Tipo de Estrada'].mode()[0])\n",
    "df['Tipo de Colisão'] = df['Tipo de Colisão'].fillna(df['Tipo de Colisão'].mode()[0])\n",
    "df['Movimento do Veículo'] = df['Movimento do Veículo'].fillna(df['Movimento do Veículo'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    if col != 'Período':\n",
    "        value_counts = df[col].value_counts(normalize=True)\n",
    "        unique_values[col] = [f\"{value}: {count*100:.2f}%\" for value, count in zip(value_counts.index, value_counts.values)]\n",
    "\n",
    "print(tabulate(unique_values, headers = 'keys', tablefmt = 'orgtbl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary target + mapping ordenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorizar_periodo(time):\n",
    "    hour = time.hour\n",
    "    if hour >= 0 and hour < 7:\n",
    "        return \"Dawn\"\n",
    "    elif hour >= 7 and hour < 12:\n",
    "        return \"Morning\"\n",
    "    elif hour >= 12 and hour < 17:\n",
    "        return \"Afternoon\"\n",
    "    elif hour >= 17 and hour < 20:\n",
    "        return \"Dusk\"\n",
    "    elif hour >= 20 and hour <= 24:\n",
    "        return \"Night\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faixa_etaria_mapping = {'18-30': 2, '31-50': 3, 'Over 51': 4, 'Unknown': 0, 'Under 18': 1}\n",
    "genero_mapping = {'Male': 1, 'Female': 2, 'Unknown': 0}\n",
    "nivel_educacao_mapping = {'Junior high school': 5, 'Elementary school': 4, 'High school': 5, 'Above high school': 6,\n",
    "                            'Writing & reading': 3, 'Unknown': 0, 'Illiterate': 1}\n",
    "relacao_com_veiculo_mapping = {'Employee': 3, 'Owner': 2, 'Other': 1, 'Unknown': 0}\n",
    "experiencia_conducao_mapping = {'5-10yr': 5, '2-5yr': 4, 'Above 10yr': 6, '1-2yr': 3, 'Below 1yr': 2, 'No Licence': 1, 'unknown': 0}\n",
    "gravidade_acidente_mapping = {'Slight Injury': 0, 'Serious Injury': 1, 'Fatal injury': 1}\n",
    "\n",
    "df['Período'] = pd.to_datetime(df['Período'], format = '%H:%M:%S')\n",
    "df[\"Período\"] = df[\"Período\"].map(categorizar_periodo)\n",
    "\n",
    "periodo_mapping = {'Dawn': 0, 'Morning': 1, 'Afternoon': 2, 'Dusk': 3, 'Night': 4}\n",
    "\n",
    "df['Faixa Etária'] = df['Faixa Etária'].map(faixa_etaria_mapping)\n",
    "df['Género'] = df['Género'].map(genero_mapping)\n",
    "df['Nível de Educação'] = df['Nível de Educação'].map(nivel_educacao_mapping)\n",
    "df['Relação com o Veículo'] = df['Relação com o Veículo'].map(relacao_com_veiculo_mapping)\n",
    "df['Experiência de Condução'] = df['Experiência de Condução'].map(experiencia_conducao_mapping)\n",
    "df['Gravidade do Acidente'] = df['Gravidade do Acidente'].map(gravidade_acidente_mapping)\n",
    "df['Período'] = df['Período'].map(periodo_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_encode = ['Situação de Faixa', 'Tipo de Cruzamento', 'Tipo de Estrada', \n",
    "                    'Condições do Piso', 'Condições de Visibilidade', \n",
    "                    'Condições Meteorológicas', 'Tipo de Colisão', \n",
    "                    'Movimento do Veículo', 'Causa do Acidente']\n",
    "\n",
    "# fazer enconder das que não se podem ordenar\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for column in columns_to_encode:\n",
    "    df[column] = label_encoder.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = {}\n",
    "\n",
    "for col in df.columns:\n",
    "    value_counts = df[col].value_counts(normalize=True)\n",
    "    unique_values[col] = [f\"{value}: {count*100:.2f}%\" for value, count in zip(value_counts.index, value_counts.values)]\n",
    "\n",
    "print(tabulate(unique_values, headers = 'keys', tablefmt = 'orgtbl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = df.copy()\n",
    "df_2 = df.copy()\n",
    "df_3 = df.copy()\n",
    "df_4 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Divisão por experiências:\n",
    "\n",
    " *  exp 1 - distribuir a proporção dos valores nulos (>4000) pelos restantes valores, mantendo a proporção\n",
    " *  exp 2 - eliminar colunas com valores nulos > 3500\n",
    " *  exp 3 - substituir valores nulos por valores em que a coluna da gravidade é a mesma (tendo em conta a sua moda)\n",
    " *  exp 4 - aplicar método do KNN às colunas com valores nulos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.1 - distribuir a proporção dos valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos as colunas que queremos alterar\n",
    "colunas_a_substituir = ['Idade do Veículo', 'Tipo de Vítima', 'Género da Vítima', 'Faixa Etária da Vítima', 'Gravidade da Vítima']\n",
    "\n",
    "for coluna in colunas_a_substituir:\n",
    "\n",
    "    no_valores_nulos = df_1[coluna].isnull().sum()\n",
    "\n",
    "    # proporção de valores não nulos para cada categoria\n",
    "    proporcoes_categoria = df_1[coluna].value_counts(normalize = True)\n",
    "\n",
    "    # calcular o número de valores nulos a serem distribuídos para cada categoria\n",
    "    distribuicao_nulos = (proporcoes_categoria * no_valores_nulos).round().astype(int)\n",
    "\n",
    "    # neste caso, damos assign a cada valor nulo de forma RANDOM\n",
    "    indices_nulo = df_1[df_1[coluna].isnull()].index\n",
    "    for category, count in distribuicao_nulos.items():\n",
    "        sample_indices = np.random.choice(indices_nulo, size = count, replace = False)\n",
    "        df_1.loc[sample_indices, coluna] = category\n",
    "\n",
    "    # preencher algum valor nulo que falte, RANDOM\n",
    "    remaining_null_indices = df_1[df_1[coluna].isnull()].index\n",
    "    remaining_categories = list(proporcoes_categoria.index)\n",
    "    for index in remaining_null_indices:\n",
    "        df_1.at[index, coluna] = np.random.choice(remaining_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_values = {}\n",
    "for column in df_1.columns:\n",
    "    counts = df_1[column].value_counts(normalize = True, dropna = False) * 100\n",
    "    percentage_values[column] = counts\n",
    "\n",
    "for column, percentages in percentage_values.items():\n",
    "    print(percentages)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.2 - eliminar colunas com valores nulos > 3500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas_valores_nulos = ['Idade do Veículo', 'Tipo de Vítima', 'Género da Vítima', 'Faixa Etária da Vítima', 'Gravidade da Vítima']\n",
    "\n",
    "df_2.drop(colunas_valores_nulos, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.3 - substituir valores nulos por valores em que a coluna da gravidade é a mesma (tendo em conta a sua moda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definimos as colunas que queremos alterar\n",
    "colunas_a_substituir = ['Idade do Veículo', 'Tipo de Vítima', 'Género da Vítima', 'Faixa Etária da Vítima', 'Gravidade da Vítima']\n",
    "\n",
    "percentage_values = {}\n",
    "for column in colunas_a_substituir:\n",
    "    counts = df_3[column].value_counts(normalize = True, dropna = False) * 100\n",
    "    percentage_values[column] = counts\n",
    "\n",
    "for column, percentages in percentage_values.items():\n",
    "    print(percentages)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target column\n",
    "target_column = 'Gravidade do Acidente'\n",
    "\n",
    "# Iterate over each column with null values\n",
    "for column in colunas_a_substituir:\n",
    "    \n",
    "    # Calculate the proportions of each value in the column for each category in the target column\n",
    "    proportions = df_3.groupby(target_column)[column].value_counts(normalize=True)\n",
    "    \n",
    "    # For each null value in the column, replace it based on the proportions\n",
    "    for index, row in df_3[df_3[column].isnull()].iterrows():\n",
    "        \n",
    "        # Get the proportions for the target value of this row\n",
    "        target_value = row[target_column]\n",
    "        target_proportions = proportions[target_value]\n",
    "        \n",
    "        # Sample from the proportions to replace the null value\n",
    "        new_value = target_proportions.sample(weights=target_proportions).index[0]\n",
    "        \n",
    "        # Replace the null value\n",
    "        df_3.at[index, column] = new_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_values = {}\n",
    "for column in colunas_a_substituir:\n",
    "    counts = df_3[column].value_counts(normalize = True, dropna = False) * 100\n",
    "    percentage_values[column] = counts\n",
    "\n",
    "for column, percentages in percentage_values.items():\n",
    "    print(percentages)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3.4 - aplicar método do KNN às colunas com valores nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer()\n",
    "\n",
    "# Get the columns with null values\n",
    "colunas_a_substituir = df_4.columns[df_4.isnull().any()]\n",
    "\n",
    "# Iterate over each column with null values\n",
    "for column in colunas_a_substituir:\n",
    "    # If the column is categorical, encode it using one-hot encoding or ordinal encoding\n",
    "    if df_4[column].dtype == 'object':\n",
    "        # Convert 'NAType' to NaN\n",
    "        df_4[column] = df_4[column].replace('NAType', np.nan)\n",
    "        # Handle missing values with a placeholder\n",
    "        df_4[column] = df_4[column].fillna('missing')\n",
    "        \n",
    "        # Apply ordinal encoding\n",
    "        encoder = OrdinalEncoder()\n",
    "        encoded_values = encoder.fit_transform(df_4[[column]])\n",
    "        encoded_df = pd.DataFrame(encoded_values, columns=[column], index=df_4.index)\n",
    "        # Drop the original categorical column and concatenate the encoded columns\n",
    "        df_4 = pd.concat([df_4.drop(column, axis=1), encoded_df], axis=1)\n",
    "    else:\n",
    "        # Extract the column data\n",
    "        X = df_4.dropna(subset=[column], axis=0).drop(colunas_a_substituir, axis=1)\n",
    "        y = df_4.dropna(subset=[column], axis=0)[column]\n",
    "        X_with_null = df_4[df_4[column].isnull()].drop(colunas_a_substituir, axis=1)\n",
    "        \n",
    "        # Apply KNN imputation\n",
    "        imputer.fit(X, y)\n",
    "        imputed_values = imputer.transform(X_with_null)\n",
    "        \n",
    "        # Update the dataframe with imputed values\n",
    "        df_4.loc[X_with_null.index, column] = imputed_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentage_values = {}\n",
    "for column in df_4.columns:\n",
    "    counts = df_4[column].value_counts(normalize = True, dropna = False) * 100\n",
    "    percentage_values[column] = counts\n",
    "\n",
    "for column, percentages in percentage_values.items():\n",
    "    print(percentages)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4 - Fazer matriz de correlação e importância de variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após o pré-processamento, decidimos ficar com a terceira experiência, pois é a que apresenta melhores resultados, como podemos observar através da matriz de correlação entre as variáveis.\n",
    "\n",
    "Aplicamos ainda o mapping às colunas anteriormente nulas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_3.copy()\n",
    "\n",
    "idade_do_veiculo_mapping = {'Unknown': 0, '2-5yrs': 3, 'Above 10yr': 5, '5-10yrs': 4 , '1-2yr': 2, 'Below 1yr': 1}\n",
    "tipo_vitima_mapping = {'Driver or rider': 1, 'Pedestrian': 2, 'Passenger': 3}\n",
    "genero_vitima_mapping = {'Male': 1, 'Female': 0}\n",
    "faixa_etaria_vitima_mapping = {'18-30': 3, '31-50': 4, 'Under 18': 2, 'Over 51': 5, '5': 1}\n",
    "\n",
    "df['Idade do Veículo'] = df['Idade do Veículo'].map(idade_do_veiculo_mapping)\n",
    "df['Tipo de Vítima'] = df['Tipo de Vítima'].map(tipo_vitima_mapping)\n",
    "df['Género da Vítima'] = df['Género da Vítima'].map(genero_vitima_mapping)\n",
    "df['Faixa Etária da Vítima'] = df['Faixa Etária da Vítima'].map(faixa_etaria_vitima_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "plt.figure(figsize = (20, 15))\n",
    "sns.heatmap(correlation_matrix, annot = True, cmap = 'vlag', fmt = \".3f\", linewidths = 1, square=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De seguida, analisamos a importância de cada feature para o modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-1].values # todas as features\n",
    "Y =  df.iloc[:, -1].values # feature target\n",
    "\n",
    "modelo = ExtraTreesClassifier()\n",
    "modelo.fit(X, Y)\n",
    "\n",
    "feature_importances = pd.Series(modelo.feature_importances_, index = df.columns[:-1])\n",
    "feature_importances.nlargest(10).plot(kind='barh')  # mostrar as 10 features mais importantes\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste ponto, dividimos os dados em sets de treino e de teste, utilizando 30% dos dados para teste e os restantes para teino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.30, random_state = 30)\n",
    "\n",
    "print(\"Número de exemplos nos dados de treino: \", X_train.shape[0])\n",
    "print(\"Número de exemplos nos dados de teste: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primeiro vamos estanderizar as nossas features. Isso é feito para garantir que todas as características tenham a mesma escala, o que pode melhorar o desempenho do algoritmo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
